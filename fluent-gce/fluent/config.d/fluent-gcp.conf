# ------------------------------------------------------------------------------
# syslog
# ------------------------------------------------------------------------------
# <source>
#   @type tail
#   format none
#   path /var/log/syslog
#   pos_file /var/tmp/fluentd.syslog.pos
#   read_from_head true
#   rotate_wait 10s
#   tag vm.syslog
# </source>

# ------------------------------------------------------------------------------
# Rails
# ------------------------------------------------------------------------------
<source>
  @type tail
  format multiline
  format_firstline /^.,/
  format1 /^., \[[^ ]* (?<process_id>[^ ]*)\][ ]+(?<severity>[^ ]*) -- : \[(?<request_id>[^ ]*)\] (?<message>.*)$/
  multiline_flush_interval 3s
  time_format %Y-%m-%dT%H:%M:%S.%N
  path /var/log/rails/*.log
  pos_file /var/tmp/fluentd.rails.pos
  read_from_head true
  tag rails.raw
</source>

# ------------------------------------------------------------------------------
# Nginx access
# ------------------------------------------------------------------------------
<source>
  @type tail
  format /^(?<remote>[^ ]*) (?<host>[^ ]*) (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*) +(?<protocol>\S*))?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")? "(?<requestTime>[^\"]*)" "(?<upstreamResponseTime>[^\"]*)" (?: tracecontext="(?<traceId>[^\/\"]*)[^\"]*")?(?: timestampSeconds="(?<timestampSeconds>[^\.\"]*)\.(?<timestampNanos>[^\"]*)")?(?: x-forwarded-for="(?:-|(?<realClientIP>[^\,\"-]+),?)[^\"]*")?.*$/
  time_format %d/%b/%Y:%H:%M:%S %z
  path /var/log/nginx/access.log
  pos_file /var/tmp/fluentd.nginx-access.pos
  read_from_head true
  rotate_wait 10s
  tag nginx.unmatched
</source>

<filter nginx.unmatched>
  @type record_transformer
  enable_ruby
  <record>
    # If realClientIP is set, use that as the remote.
    remote ${record["realClientIP"] or record["remote"]}
  </record>
</filter>

# Sort nginx logs into reasonable buckets.
<match nginx.unmatched>
  @type copy
  <store>
    @type rewrite_tag_filter
    rewriterule1 path /_ah/health nginx.health_check
    rewriterule2 path /_ah/vm_health nginx.health_check
    rewriterule3 path /liveness_check nginx.health_check
    rewriterule4 path /readiness_check nginx.health_check
    rewriterule5 path .* nginx.request
  </store>
  <store>
    @type rewrite_tag_filter
    rewriterule1 path /_ah/health nginx.health_check_copy
    rewriterule2 path /_ah/vm_health nginx.health_check_copy
    rewriterule3 path /liveness_check nginx.health_check_copy
    rewriterule4 path /readiness_check nginx.health_check_copy
  </store>
</match>

# Save health checks to file on disk every minute
# Path is path + time + .log
<match nginx.health_check_copy>
  @type file
  flush_at_shutdown true
  flush_interval 2m
  path /var/log/fluent/health_check
  time_slice_format %Y%m%d%H%M
</match>

# ------------------------------------------------------------------------------
# Nginx error
# ------------------------------------------------------------------------------
<source>
  @type tail
  path /var/log/nginx/error.log
  pos_file /var/tmp/fluentd.nginx-error.pos
  format multiline
  format_firstline /^\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2} \[\w+\] \d+.\d+: /
  format1 /^(?<time>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) (?<message>.*)/
  multiline_flush_interval 3s
  tag nginx.error
</source>

# Link /var/lib/docker/containers we want to save to /var/log/docker_containers
<source>
  @type tail
  path /var/log/saved_docker/*/*-json.log
  pos_file /var/tmp/saved-docker.pos
  time_format %Y-%m-%dT%H:%M:%S
  tag docker.*
  read_from_head true
  format json
</source>

# Transform nginx request (access) log fields into a nested hash suitable
# for consumption by Cloud Logging UI's request_log formatters.
# This extra step is necessary because fluentd regex input cannot be nested.
# An alternative would be to change nginx config to output valid json with
# the correct nested hash keys.
<filter nginx.{health_check,request}>
  @type record_transformer
  <record>
    httpRequest {
      "requestMethod": "${record[\"method\"]}",
      "referer": "${record[\"referer\"]}",
      "remoteIp": "${record[\"remote\"]}",
      "userAgent": "${record[\"agent\"]}",
      "requestUrl": "${record[\"path\"]}",
      "responseSize": "${record[\"size\"]}",
      "latency": "${record[\"requestTime\"]}s",
      "status": "${record[\"code\"]}",
      "protocol": "${record[\"protocol\"]}"
    }
  </record>
  renew_record true
  keep_keys time,timestampSeconds,timestampNanos,requestTime,upstreamResponseTime,instanceName
</filter>

# Docker container logs go through the from_docker container
# <match docker.var.log.saved_docker.*.*.log>
#   @type from_docker
#   stdout_tag raw.stdout
#   stderr_tag raw.stderr
# </match>

# Detect exceptions from stdout and stderr of Docker containers.
<match raw.**>
  @type detect_exceptions
  remove_tag_prefix raw
  message message
  multiline_flush_interval 5
  max_bytes 50000
  max_lines 500
</match>

<match **>
  @type google_cloud
  # Detect and parse JSON written to text logs.
  detect_json true
  buffer_chunk_limit 1m
  flush_interval 5s
  # Never wait longer than 5 minutes between retries
  max_retry_wait 300
  num_threads 8
  disable_retry_limit
  # Send these fields as labels instead of in the struct_payload
  label_map {

  }
  use_grpc true
</match>
